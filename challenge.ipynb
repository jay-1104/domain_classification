{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59837c4f",
   "metadata": {},
   "source": [
    "## Domain Classification Challenge\n",
    "\n",
    "**Goal:** Build a classifier to detect valid main domains from a given dataset with minimal false positives.\n",
    "\n",
    "**Note:** The subsidiaries listed in the last column of the CSV serve as the source of truth. These entries are pseudonymized in the dataset. The objective is to identify any main domains associated with these subsidiaries.\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "- Precision (especially minimizing false positives)\n",
    "- Use of multiple classification techniques\n",
    "- Model interpretability\n",
    "- Clean and modular code\n",
    "\n",
    "**Dataset:**\n",
    "- Features include domain-level characteristics, text patterns etc.\n",
    "- Target: `label` (1 = valid, 0 = invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485eae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61bec14a",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "- Precision Score\n",
    "- Confusion Matrix\n",
    "- False Positive Rate\n",
    "- ROC-AUC (Optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4ecd6e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Compare all models based on precision and FPR.\n",
    "\n",
    "Discuss:\n",
    "- Which model performed best and why?\n",
    "- What could be done better with more time/data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce8056",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f55735c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  New features added: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, re, json, time\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "import dns.resolver, whois, tldextract, pycountry\n",
    "from pathlib import Path\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load file\n",
    "df = pd.read_csv(\"domains.csv\")\n",
    "# --------------------------------------------------\n",
    "# 1. URL-level lexical stats\n",
    "def url_stats(u):\n",
    "    if pd.isna(u): \n",
    "        return pd.Series({\"url_length\": np.nan,\n",
    "                          \"path_length\": np.nan,\n",
    "                          \"path_token_cnt\": np.nan,\n",
    "                          \"path_token_len_avg\": np.nan,\n",
    "                          \"path_token_len_max\": np.nan})\n",
    "    p = urlparse(u if \"://\" in u else \"http://\"+u)   # tolerate scheme-less\n",
    "    path = p.path or \"/\"\n",
    "    tokens = [t for t in path.split(\"/\") if t]\n",
    "    token_lens = list(map(len, tokens)) or [0]\n",
    "    return pd.Series({\n",
    "        \"url_length\": len(u),\n",
    "        \"path_length\": len(path),\n",
    "        \"path_token_cnt\": len(tokens),\n",
    "        \"path_token_len_avg\": np.mean(token_lens),\n",
    "        \"path_token_len_max\": np.max(token_lens)\n",
    "    })\n",
    "\n",
    "url_col = \"url\" if \"url\" in df.columns else None\n",
    "if url_col:\n",
    "    df = pd.concat([df, df[url_col].apply(url_stats)], axis=1)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Domain token stats\n",
    "def dom_token_stats(dom):\n",
    "    parts = re.split(r\"[.-]\", str(dom))\n",
    "    lengths = list(map(len, parts)) or [0]\n",
    "    return pd.Series({\n",
    "        \"dom_token_cnt\": len(parts),\n",
    "        \"dom_token_len_avg\": np.mean(lengths),\n",
    "        \"dom_token_len_max\": np.max(lengths)\n",
    "    })\n",
    "df = pd.concat([df, df[\"domain\"].apply(dom_token_stats)], axis=1)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Site-age (months) and update-age (months)  \n",
    "for raw_col, new_col in [\n",
    "    (\"domain_whois.creation_date\", \"site_age_months\"),\n",
    "    (\"domain_whois.updated_date\",  \"update_age_months\")\n",
    "]:\n",
    "    if raw_col in df.columns:\n",
    "        dt = pd.to_datetime(df[raw_col], errors=\"coerce\")\n",
    "        months = (dt.dt.year - 1970) * 12 + dt.dt.month\n",
    "        df[new_col] = months\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Rogue-index helper\n",
    "def rogue_index(series, labels):\n",
    "    tab = pd.crosstab(series, labels)\n",
    "    M = labels.sum()\n",
    "    B = len(labels) - M\n",
    "    ri = (tab[1]/M) / ((tab[1]/M) + (tab[0]/B))\n",
    "    return ri.fillna(0).to_dict()\n",
    "\n",
    "# Rogue for name-server (nets_names), registrar, ASN\n",
    "if \"ip_whois.nets_names\" in df.columns:\n",
    "    ns_map = rogue_index(df[\"ip_whois.nets_names\"], df[\"label\"])\n",
    "    df[\"ns_rogue_idx\"] = df[\"ip_whois.nets_names\"].map(ns_map).fillna(0)\n",
    "\n",
    "if \"domain_whois.registrar\" in df.columns:\n",
    "    reg_map = rogue_index(df[\"domain_whois.registrar\"], df[\"label\"])\n",
    "    df[\"registrar_rogue_idx\"] = df[\"domain_whois.registrar\"].map(reg_map).fillna(0)\n",
    "\n",
    "if \"ip_whois.asn_descriptions\" in df.columns:\n",
    "    asn_map = rogue_index(df[\"ip_whois.asn_descriptions\"], df[\"label\"])\n",
    "    df[\"asn_rogue_idx\"] = df[\"ip_whois.asn_descriptions\"].map(asn_map).fillna(0)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. 97.5 % clipping + [0,1] normalisation for new continuous cols\n",
    "cont_cols = [\n",
    "    \"url_length\", \"path_length\", \"path_token_cnt\",\n",
    "    \"path_token_len_avg\", \"path_token_len_max\",\n",
    "    \"dom_token_cnt\", \"dom_token_len_avg\", \"dom_token_len_max\",\n",
    "    \"site_age_months\", \"update_age_months\"\n",
    "]\n",
    "for c in cont_cols:\n",
    "    if c in df.columns:\n",
    "        upper = np.nanpercentile(df[c], 97.5)\n",
    "        df[c] = np.where(df[c] > upper, upper, df[c])\n",
    "        rng = df[c].max() - df[c].min()\n",
    "        if rng > 0:\n",
    "            df[c] = (df[c] - df[c].min()) / rng\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. Save / preview\n",
    "df_enriched = df.copy()\n",
    "df_enriched.to_csv(\"domains_enriched.csv\", index=False)\n",
    "print(\"New features added:\", sorted(set(df_enriched.columns) - set(df.columns) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd1a30ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  'domain_whois.creation_date' column not found – feature skipped\n",
      "⚠️  'domain_whois.updated_date' column not found – feature skipped\n",
      "⚠️  No URL/path column — path stats skipped\n",
      "✅  domains_enriched_full.csv created with new host-based features\n"
     ]
    }
   ],
   "source": [
    "# Adding some more host based features as it feels important to have those features on feature engineering\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 0.  Load enriched dataframe \n",
    "df = pd.read_csv(\"domains_enriched.csv\")\n",
    "\n",
    "# Cache folders so we never hammer external services twice\n",
    "CACHE_DIR = Path(\"lookups_cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def cached_json(path):\n",
    "    if path.exists():\n",
    "        return json.loads(path.read_text())\n",
    "    return {}\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1. Site-age and last-update age \n",
    "for raw, new in [(\"domain_whois.creation_date\", \"site_age_months\"),\n",
    "                 (\"domain_whois.updated_date\",  \"update_age_months\")]:\n",
    "    if raw in df.columns:\n",
    "        dt = pd.to_datetime(df[raw], errors=\"coerce\")\n",
    "        months = (dt.dt.year - 1970) * 12 + dt.dt.month\n",
    "        df[new]  = (months - months.min()) / (months.quantile(0.975) - months.min())\n",
    "    else:\n",
    "        print(f\"'{raw}' column not found – feature skipped\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2. Path-level lexical stats\n",
    "url_source = \"url\" if \"url\" in df.columns else None   \n",
    "if url_source:\n",
    "    def path_stats(u):\n",
    "        p  = urlparse(u if \"://\" in u else \"http://\"+u)\n",
    "        toks = [t for t in p.path.split(\"/\") if t]\n",
    "        lens = [len(t) for t in toks] or [0]\n",
    "        return pd.Series({\n",
    "            \"path_length\"        : len(p.path),\n",
    "            \"path_token_cnt\"     : len(toks),\n",
    "            \"path_token_len_avg\" : np.mean(lens),\n",
    "            \"path_token_len_max\" : np.max(lens)\n",
    "        })\n",
    "    new_path = df[url_source].fillna(\"\").apply(path_stats)\n",
    "    # clip + scale to [0,1]\n",
    "    for c in new_path.columns:\n",
    "        upper      = new_path[c].quantile(0.975)\n",
    "        new_path[c] = np.minimum(new_path[c], upper)\n",
    "        new_path[c] = (new_path[c] - new_path[c].min())/(upper - new_path[c].min())\n",
    "    df = pd.concat([df, new_path], axis=1)\n",
    "else:\n",
    "    print(\" No URL/path column — path stats skipped\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3. DNS counts  (NS & MX)   – uses dnspython with simple caching\n",
    "def dns_count(domain, rtype):\n",
    "    cache_file = CACHE_DIR / f\"{rtype}_{domain}.json\"\n",
    "    cache      = cached_json(cache_file)\n",
    "    if cache:\n",
    "        return cache[\"count\"]\n",
    "    try:\n",
    "        answers = dns.resolver.resolve(domain, rtype, lifetime=3.0)\n",
    "        count   = len(answers)\n",
    "    except dns.exception.DNSException:\n",
    "        count = 0\n",
    "    cache_file.write_text(json.dumps({\"count\": count}))\n",
    "    return count\n",
    "\n",
    "df[\"ns_count\"] = df[\"domain\"].apply(lambda d: dns_count(d, \"NS\"))\n",
    "df[\"mx_count\"] = df[\"domain\"].apply(lambda d: dns_count(d, \"MX\"))\n",
    "\n",
    "# clip + log-scale\n",
    "for col in [\"ns_count\", \"mx_count\"]:\n",
    "    df[col] = np.log1p(df[col])\n",
    "    df[col] = df[col] / df[col].quantile(0.975)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4. Country of ASN   (ISO-2 → ISO-3 for modelling)\n",
    "def asn_country(asn_desc):\n",
    "    if pd.isna(asn_desc):\n",
    "        return \"UNK\"\n",
    "    match = re.match(r\"([A-Z]{2})\\s\", asn_desc.upper())\n",
    "    iso2  = match.group(1) if match else \"ZZ\"\n",
    "    try:\n",
    "        return pycountry.countries.get(alpha_2=iso2).alpha_3\n",
    "    except:\n",
    "        return \"UNK\"\n",
    "\n",
    "if \"ip_whois.asn_descriptions\" in df.columns:\n",
    "    df[\"asn_country\"] = df[\"ip_whois.asn_descriptions\"].apply(asn_country)\n",
    "else:\n",
    "    print(\"ASN description column missing — country_of_ASN skipped\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5. Rogue-index for registrar (if not already)\n",
    "def rogue_index(series, labels):\n",
    "    tab = pd.crosstab(series, labels)\n",
    "    M, B = labels.sum(), len(labels) - labels.sum()\n",
    "    return ((tab[1]/M) / ((tab[1]/M) + (tab[0]/B))).fillna(0).to_dict()\n",
    "\n",
    "if \"domain_whois.registrar\" in df.columns and \"registrar_rogue_idx\" not in df:\n",
    "    ri = rogue_index(df[\"domain_whois.registrar\"], df[\"label\"])\n",
    "    df[\"registrar_rogue_idx\"] = df[\"domain_whois.registrar\"].map(ri).fillna(0)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 6. Save\n",
    "df.to_csv(\"domains_enriched_full.csv\", index=False)\n",
    "print(\"domains_enriched_full.csv created with new host-based features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d35d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b682ecf",
   "metadata": {},
   "source": [
    "## Preprocessing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5439f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(df, label_col=\"label\"):\n",
    "    \"\"\"\n",
    "    Apply 3-stage preprocessing:\n",
    "      1. 97.5-percentile clipping + min-max scaling on continuous features\n",
    "      2. keep rogue-index columns as-is (they're already 0-1)\n",
    "      3. ensure true binary columns are 0/1 int8\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    df_proc = df.copy()\n",
    "    \n",
    "    # ----------------------------------------------------------\n",
    "    # 1. Identify feature groups\n",
    "    rogue_pattern   = re.compile(r\"_rogue_idx$\")\n",
    "    binary_pattern  = re.compile(r\"(?:_flag$|_binary$|dom_country_tld$)\")\n",
    "    \n",
    "    numeric_cols = df_proc.select_dtypes(exclude=\"object\").columns.drop(label_col)\n",
    "    rogue_cols   = [c for c in numeric_cols if rogue_pattern.search(c)]\n",
    "    binary_cols  = [c for c in numeric_cols if binary_pattern.search(c)]\n",
    "    cont_cols    = [c for c in numeric_cols if c not in rogue_cols + binary_cols]\n",
    "    \n",
    "    # ----------------------------------------------------------\n",
    "    # 2. Clip & scale continuous variables\n",
    "    for col in cont_cols:\n",
    "        series = df_proc[col].astype(float)\n",
    "        upper  = np.nanpercentile(series, 97.5)\n",
    "        lower  = np.nanpercentile(series,  0.0)   # keep min (usually 0)\n",
    "        series_clipped = series.clip(lower, upper)\n",
    "        rng = upper - lower\n",
    "        if rng == 0:\n",
    "            df_proc[col] = 0.0   # constant column\n",
    "        else:\n",
    "            df_proc[col] = (series_clipped - lower) / rng\n",
    "    \n",
    "    # ----------------------------------------------------------\n",
    "    # 3. Binary columns cast to int8 (0/1)\n",
    "    for col in binary_cols:\n",
    "        df_proc[col] = df_proc[col].astype(int).clip(0, 1).astype(\"int8\")\n",
    "    \n",
    "    # Rogue-index columns left untouched, but fill NaNs with 0\n",
    "    df_proc[rogue_cols] = df_proc[rogue_cols].fillna(0.0)\n",
    "    \n",
    "    return df_proc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895f90dc",
   "metadata": {},
   "source": [
    "### Handeling Text based featuers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b68aa770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\d_drive\\VS_CODE\\impressioms\\vk\\lib\\site-packages\\sklearn\\utils\\extmath.py:1050: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\d_drive\\VS_CODE\\impressioms\\vk\\lib\\site-packages\\sklearn\\utils\\extmath.py:1055: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\d_drive\\VS_CODE\\impressioms\\vk\\lib\\site-packages\\sklearn\\utils\\extmath.py:1075: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  domains_feature_ready.csv written — shape: (1090, 525)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature-engineering pipeline\n",
    "────────────────────────────\n",
    "1.  Loads `domains_enriched_full.csv`\n",
    "2.  Normalises / clips every continuous numeric column to [0,1]\n",
    "3.  Leaves rogue-index columns unchanged (already 0-1)\n",
    "4.  Casts obvious binary flags to int8\n",
    "5.  Vectorises every text column:\n",
    "      • domain  → char-TF-IDF 3-5-grams\n",
    "      • huge blobs → word Hashing + SafeSVD(50)\n",
    "      • remaining text → word TF-IDF + SafeSVD(25)\n",
    "   SafeSVD skips reduction if the TF-IDF block has <2 features.\n",
    "6.  Concatenates everything into a single numeric matrix\n",
    "7.  Writes `domains_feature_ready.csv` (features + label) — ready for\n",
    "    feature selection or model training.\n",
    "\"\"\"\n",
    "\n",
    "# ───────────────────────────────────────────────────────── Load data\n",
    "import pandas as pd, numpy as np, re\n",
    "from pathlib import Path\n",
    "\n",
    "df = pd.read_csv(\"domains_enriched_full.csv\")\n",
    "label_col = \"label\"\n",
    "\n",
    "# ───────────────────────────────────────────────────────── Numeric preprocessing\n",
    "rogue_pattern  = re.compile(r\"_rogue_idx$\")\n",
    "binary_pattern = re.compile(r\"(?:_flag$|_binary$|dom_country_tld$)\")\n",
    "\n",
    "num_cols_all   = df.select_dtypes(exclude=\"object\").columns.drop(label_col)\n",
    "rogue_cols     = [c for c in num_cols_all if rogue_pattern.search(c)]\n",
    "binary_cols    = [c for c in num_cols_all if binary_pattern.search(c)]\n",
    "cont_cols      = [c for c in num_cols_all if c not in rogue_cols + binary_cols]\n",
    "\n",
    "def clip_scale(col):\n",
    "    upper = col.quantile(0.975)\n",
    "    lower = col.min()\n",
    "    col   = col.clip(lower, upper)\n",
    "    rng   = upper - lower\n",
    "    return (col - lower)/rng if rng else 0.0\n",
    "\n",
    "df[cont_cols]  = df[cont_cols].apply(clip_scale, axis=0)\n",
    "df[binary_cols]= df[binary_cols].astype(int).clip(0,1).astype(\"int8\")\n",
    "df[rogue_cols] = df[rogue_cols].fillna(0.0)\n",
    "\n",
    "# ───────────────────────────────────────────────────────── Text preprocessing\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import sparse\n",
    "\n",
    "class SafeSVD(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_components=25, min_feats=2):\n",
    "        self.n_components = n_components; self.min_feats = min_feats; self.svd_ = None\n",
    "    def fit(self, X, y=None):\n",
    "        n_feats = X.shape[1]\n",
    "        if n_feats >= self.min_feats:\n",
    "            self.svd_ = TruncatedSVD(\n",
    "                n_components=min(self.n_components, n_feats-1),\n",
    "                random_state=42).fit(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        if self.svd_ is None:      # keep original\n",
    "            return X.toarray() if sparse.isspmatrix(X) else X\n",
    "        return self.svd_.transform(X)\n",
    "\n",
    "def block_tfidf(analyzer, ngram, max_feats, svd_comp):\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(analyzer=analyzer, ngram_range=ngram,\n",
    "                                  max_features=max_feats, min_df=2,\n",
    "                                  sublinear_tf=True)),\n",
    "        (\"svd\", SafeSVD(svd_comp))\n",
    "    ])\n",
    "\n",
    "text_cols = [c for c in df.columns if df[c].dtype == object and c != label_col]\n",
    "df[text_cols] = df[text_cols].fillna(\"\")\n",
    "\n",
    "big_blobs = [\"contents_info\", \"ip_whois.nets_description\"]\n",
    "transformers = [\n",
    "    (\"char_dom\", block_tfidf(\"char\", (3,5), 20000, 40), \"domain\")\n",
    "]\n",
    "\n",
    "for col in big_blobs:\n",
    "    if col in text_cols:\n",
    "        transformers.append((\n",
    "            f\"hash_{col}\",\n",
    "            Pipeline([\n",
    "                (\"hash\", HashingVectorizer(analyzer=\"word\",\n",
    "                                           n_features=2**18,\n",
    "                                           alternate_sign=False)),\n",
    "                (\"svd\", SafeSVD(50))\n",
    "            ]),\n",
    "            col\n",
    "        ))\n",
    "        text_cols.remove(col)\n",
    "\n",
    "for col in [c for c in text_cols if c != \"domain\"]:\n",
    "    transformers.append((f\"tfidf_{col}\",\n",
    "                         block_tfidf(\"word\", (1,2), 8000, 25),\n",
    "                         col))\n",
    "\n",
    "transformers.append((\"num\", StandardScaler(with_mean=False),\n",
    "                     cont_cols + rogue_cols + binary_cols))\n",
    "\n",
    "ct = ColumnTransformer(transformers, sparse_threshold=0.3)\n",
    "\n",
    "# ───────────────────────────────────────────────────────── Fit-transform\n",
    "X_numeric = ct.fit_transform(df)\n",
    "y         = df[label_col].values\n",
    "\n",
    "# ───────────────────────────────────────────────────────── Column names\n",
    "colnames = []\n",
    "\n",
    "for name, trans, cols in ct.transformers_:\n",
    "    if name == \"num\":\n",
    "        # numeric passthrough: just keep original column names\n",
    "        colnames.extend(cols)\n",
    "        continue\n",
    "\n",
    "    if isinstance(trans, str) and trans == \"drop\":\n",
    "        # nothing came out of this transformer\n",
    "        continue\n",
    "\n",
    "    # SafeSVD pipeline\n",
    "    fitted = trans.named_steps[\"svd\"].svd_\n",
    "    if fitted is None:          # SVD skipped → single feature\n",
    "        n_comp = 1\n",
    "    else:\n",
    "        n_comp = fitted.n_components\n",
    "    colnames.extend([f\"{name}_{i:02d}\" for i in range(n_comp)])\n",
    "# ───────────────────────────────────────────────────────── Save\n",
    "df_featready = pd.DataFrame(X_numeric, columns=colnames)\n",
    "df_featready[label_col] = y\n",
    "df_featready.to_csv(\"domains_feature_ready.csv\", index=False)\n",
    "print(\"domains_feature_ready.csv written — shape:\", df_featready.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4c04b",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70a899cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"domains_feature_ready.csv\")\n",
    "X = df.drop(columns=\"label\").values\n",
    "y = df[\"label\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29016f",
   "metadata": {},
   "source": [
    "### LASSO Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba9ca2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 0.1 | Features kept: 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import numpy as np\n",
    "\n",
    "# ── 1. choose best C on imputed data\n",
    "imp = SimpleImputer(strategy=\"constant\", fill_value=0.0).fit(X)\n",
    "X_imp = imp.transform(X)\n",
    "\n",
    "best_c, best_auc = None, -1\n",
    "for C in [0.1, 0.2, 0.3, 0.5, 1]:\n",
    "    auc = cross_val_score(\n",
    "        LogisticRegression(\n",
    "            penalty=\"l1\", solver=\"saga\", max_iter=20000, tol=1e-3,\n",
    "            class_weight=\"balanced\", C=C\n",
    "        ),\n",
    "        X_imp, y, cv=5, scoring=\"roc_auc\"\n",
    "    ).mean()\n",
    "    if auc > best_auc:\n",
    "        best_auc, best_c = auc, C\n",
    "\n",
    "# ── 2. fit final LASSO on imputed X\n",
    "lasso = LogisticRegression(\n",
    "    penalty=\"l1\", solver=\"saga\", max_iter=20000, tol=1e-3,\n",
    "    class_weight=\"balanced\", C=best_c\n",
    ").fit(X_imp, y)\n",
    "\n",
    "# ── 3. select features (threshold = mean(|β|))\n",
    "sfm   = SelectFromModel(lasso, prefit=True, threshold=\"mean\")\n",
    "mask  = sfm.get_support()\n",
    "X_lasso = sfm.transform(X_imp)\n",
    "\n",
    "print(f\"Best C: {best_c} | Features kept: {mask.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf5d616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...................... JUST A TRY WITH THOSE 4 SELECTED FEATURES ON RANDOM FOREST ................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff55c61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['lookup_count', 'dom_token_cnt', 'ns_rogue_idx', 'asn_rogue_idx']\n",
      "{'Accuracy': 0.9724770642201835, 'Precision': 0.9272727272727272, 'Recall': 0.9622641509433962, 'F1': 0.9444444444444444, 'ROC-AUC': 0.9953116066323614, 'TN': 161, 'FP': 4, 'FN': 2, 'TP': 51}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "df = pd.read_csv(\"domains_feature_ready.csv\")\n",
    "X, y = df.drop(columns=\"label\"), df[\"label\"]\n",
    "\n",
    "# Impute NaNs → 0\n",
    "imp  = SimpleImputer(strategy=\"constant\", fill_value=0.0).fit(X)\n",
    "X_imp = imp.transform(X)\n",
    "\n",
    "# LASSO (C = 0.1) keep non-zero β\n",
    "lasso = LogisticRegression(penalty=\"l1\", solver=\"saga\", C=0.1,\n",
    "                           max_iter=20000, tol=1e-3,\n",
    "                           class_weight=\"balanced\").fit(X_imp, y)\n",
    "mask  = SelectFromModel(lasso, prefit=True, threshold=None).get_support()\n",
    "X_sel = X_imp[:, mask]\n",
    "\n",
    "print(\"Selected features:\", X.columns[mask].tolist())\n",
    "\n",
    "# RF on the 4-feature set\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_sel, y, test_size=0.2,\n",
    "                                          stratify=y, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=None,\n",
    "                            min_samples_leaf=2, class_weight=\"balanced\",\n",
    "                            random_state=42, n_jobs=-1).fit(X_tr, y_tr)\n",
    "y_pr  = rf.predict(X_te); y_pb = rf.predict_proba(X_te)[:,1]\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_pr).ravel()\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_te, y_pr),\n",
    "    \"Precision\": precision_score(y_te, y_pr, zero_division=0),\n",
    "    \"Recall\": recall_score(y_te, y_pr),\n",
    "    \"F1\": f1_score(y_te, y_pr),\n",
    "    \"ROC-AUC\": roc_auc_score(y_te, y_pb),\n",
    "    \"TN\": tn, \"FP\": fp, \"FN\": fn, \"TP\": tp\n",
    "}\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b1ae05",
   "metadata": {},
   "source": [
    "## tHE ABOVE CODE WORKS NICE. EXPLORING GA TO MAKE MODEL FURTHER BETTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e7d6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "df = pd.read_csv(\"domains_feature_ready.csv\")\n",
    "X_all, y = df.drop(columns=\"label\"), df[\"label\"]\n",
    "\n",
    "# Re-use the imputer so GA sees no NaNs\n",
    "imp   = SimpleImputer(strategy=\"constant\", fill_value=0.0).fit(X_all)\n",
    "X_imp = imp.transform(X_all)\n",
    "\n",
    "# LASSO again, but keep *all* non-zero weights (C=0.3 mild shrinkage)\n",
    "lasso = LogisticRegression(penalty=\"l1\", solver=\"saga\", C=0.3,\n",
    "                           max_iter=20000, class_weight=\"balanced\", tol=1e-3\n",
    ").fit(X_imp, y)\n",
    "mask_all = (lasso.coef_[0] != 0)\n",
    "\n",
    "mandatory = [\"lookup_count\", \"dom_token_cnt\",\n",
    "             \"ns_rogue_idx\", \"asn_rogue_idx\"]\n",
    "mand_mask = X_all.columns.isin(mandatory)\n",
    "\n",
    "# Pool that GA can toggle  (exclude mandatory)\n",
    "opt_mask   = mask_all & ~mand_mask\n",
    "X_mand     = X_imp[:, mand_mask]          # will be concatenated back later\n",
    "X_opt      = X_imp[:, opt_mask]           # GA toggles these columns\n",
    "opt_names  = X_all.columns[opt_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf42f861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
      "0  \t60    \t-53333 \t49889.1    \t0.760948   \t-100000    \n",
      "1  \t120   \t-14999.4\t35707.4    \t0.760948   \t-100000    \n",
      "2  \t120   \t0.735694\t0.0239183  \t0.78434    \t0.663272   \n",
      "3  \t120   \t0.75307 \t0.0141759  \t0.78924    \t0.719488   \n",
      "4  \t120   \t0.759047\t0.0141954  \t0.78924    \t0.723211   \n",
      "5  \t120   \t0.770273\t0.0116112  \t0.78924    \t0.750493   \n",
      "6  \t120   \t0.778921\t0.00801652 \t0.790542   \t0.755177   \n",
      "7  \t120   \t0.784034\t0.00477284 \t0.795291   \t0.769801   \n",
      "8  \t120   \t0.787741\t0.00324655 \t0.795291   \t0.780264   \n",
      "9  \t120   \t0.789722\t0.00268794 \t0.795881   \t0.784172   \n",
      "10 \t120   \t0.792394\t0.00323867 \t0.800449   \t0.787936   \n",
      "11 \t120   \t0.796135\t0.00401657 \t0.808701   \t0.788255   \n",
      "12 \t120   \t0.799399\t0.00439804 \t0.808701   \t0.788152   \n",
      "13 \t120   \t0.803301\t0.00373139 \t0.808701   \t0.795291   \n",
      "14 \t120   \t0.806579\t0.00275679 \t0.812175   \t0.799705   \n",
      "15 \t120   \t0.808344\t0.00228389 \t0.812175   \t0.805025   \n",
      "16 \t120   \t0.810507\t0.00239526 \t0.816801   \t0.806425   \n",
      "17 \t120   \t0.81188 \t0.00202317 \t0.820993   \t0.807629   \n",
      "18 \t120   \t0.812873\t0.00191226 \t0.820993   \t0.812175   \n",
      "19 \t120   \t0.813927\t0.00268162 \t0.820993   \t0.812175   \n",
      "20 \t120   \t0.816199\t0.00346781 \t0.820993   \t0.812175   \n",
      "21 \t120   \t0.818721\t0.00286943 \t0.822514   \t0.812402   \n",
      "22 \t120   \t0.821025\t0.00121682 \t0.823848   \t0.816801   \n",
      "23 \t120   \t0.821618\t0.00112143 \t0.823848   \t0.820993   \n",
      "24 \t120   \t0.822696\t0.00128709 \t0.823848   \t0.820993   \n",
      "25 \t120   \t0.823594\t0.000923346\t0.827889   \t0.820993   \n",
      "26 \t120   \t0.824235\t0.00113436 \t0.827889   \t0.823848   \n",
      "27 \t120   \t0.824664\t0.00156244 \t0.827889   \t0.823848   \n",
      "28 \t120   \t0.825901\t0.00195625 \t0.827889   \t0.823848   \n",
      "29 \t120   \t0.82765 \t0.000931856\t0.828218   \t0.823848   \n",
      "30 \t120   \t0.827922\t9.8895e-05 \t0.828218   \t0.827889   \n",
      "31 \t120   \t0.827977\t0.000145777\t0.828218   \t0.827889   \n",
      "32 \t120   \t0.828043\t0.000164458\t0.828218   \t0.827889   \n",
      "33 \t120   \t0.828158\t0.000127555\t0.828218   \t0.827889   \n",
      "34 \t120   \t0.828213\t4.22015e-05\t0.828218   \t0.827889   \n",
      "35 \t120   \t0.828218\t9.99201e-16\t0.828218   \t0.828218   \n",
      "36 \t120   \t0.828218\t9.99201e-16\t0.828218   \t0.828218   \n",
      "37 \t120   \t0.828218\t9.99201e-16\t0.828218   \t0.828218   \n",
      "38 \t120   \t0.828218\t9.99201e-16\t0.828218   \t0.828218   \n",
      "39 \t120   \t0.828218\t9.99201e-16\t0.828218   \t0.828218   \n",
      "40 \t120   \t0.828218\t9.99201e-16\t0.828218   \t0.828218   \n",
      "✅  GA kept 29 optional columns\n",
      "Total final features: 33\n",
      "Saved X / y in GA_selected_features.npz and column list in GA_selected_columns.txt\n"
     ]
    }
   ],
   "source": [
    "# ── 0. make sure GeneticSelectionCV exists ─────────────────────────────\n",
    "import subprocess, sys, importlib\n",
    "pkg = \"sklearn-genetic-opt\"\n",
    "try:\n",
    "    from sklearn_genetic import GAFeatureSelectionCV\n",
    "except ImportError:\n",
    "    print(\"🔄  upgrading sklearn-genetic-opt …\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", pkg], check=True)\n",
    "    importlib.invalidate_caches()\n",
    "    from sklearn_genetic import GAFeatureSelectionCV\n",
    "    \n",
    "\n",
    "# ── 1. load prepared numeric matrix  ───────────────────────────────────\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "df = pd.read_csv(\"domains_feature_ready.csv\")\n",
    "X, y  = df.drop(columns=\"label\"), df[\"label\"]\n",
    "\n",
    "imp   = SimpleImputer(strategy=\"constant\", fill_value=0.0).fit(X)\n",
    "X_imp = imp.transform(X)                      # NaNs → 0\n",
    "cols  = X.columns\n",
    "\n",
    "# mandatory features from LASSO\n",
    "mand_cols = [\"lookup_count\", \"dom_token_cnt\", \"ns_rogue_idx\", \"asn_rogue_idx\"]\n",
    "mand_mask = cols.isin(mand_cols)\n",
    "X_mand    = X_imp[:, mand_mask]              # always ON\n",
    "X_opt     = X_imp[:, ~mand_mask]             # GA toggles these\n",
    "opt_cols  = cols[~mand_mask]\n",
    "\n",
    "# ── 2. GA selector on optional pool  ───────────────────────────────────\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "base_est = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm\"   , SVC(kernel=\"rbf\", probability=True,\n",
    "                   class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "ga = GAFeatureSelectionCV(\n",
    "    estimator              = base_est,\n",
    "    cv                     = 3,\n",
    "    scoring                = \"roc_auc\",\n",
    "    population_size        = 60,     \n",
    "    generations            = 40,     \n",
    "    crossover_probability  = 0.97,    \n",
    "    mutation_probability   = 0.03,   \n",
    "    tournament_size        = 3,\n",
    "    elitism                = True,\n",
    "    verbose                = True,\n",
    "    n_jobs                 = -1,\n",
    "    max_features           = 30\n",
    ")\n",
    "\n",
    "ga.fit(X_opt, y)\n",
    "opt_mask = ga.support_\n",
    "print(f\"GA kept {opt_mask.sum()} optional columns\")\n",
    "\n",
    "# ── 3. concatenate mandatory + GA-chosen optional  ─────────────────────\n",
    "X_final      = np.hstack([X_mand, X_opt[:, opt_mask]])\n",
    "final_cols   = list(mand_cols) + opt_cols[opt_mask].tolist()\n",
    "np.savez(\"GA_selected_features.npz\", X=X_final, y=y)   # quick save\n",
    "pd.Series(final_cols).to_csv(\"GA_selected_columns.txt\", index=False)\n",
    "\n",
    "print(\"Total final features:\", len(final_cols))\n",
    "print(\"Saved X / y in GA_selected_features.npz and column list in GA_selected_columns.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97279c9",
   "metadata": {},
   "source": [
    "## Testing new dataframe on ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6e4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# full numeric feature matrix\n",
    "df = pd.read_csv(\"domains_feature_ready.csv\")\n",
    "X_all, y = df.drop(columns=\"label\"), df[\"label\"]\n",
    "\n",
    "# list of columns GA chose (mandatory + optional)\n",
    "final_cols = pd.read_csv(\"GA_selected_columns.txt\", header=None)[0].tolist()\n",
    "\n",
    "# subset + impute NaNs → 0\n",
    "imp = SimpleImputer(strategy=\"constant\", fill_value=0.0).fit(X_all[final_cols])\n",
    "X_sel = imp.transform(X_all[final_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84336978",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd64625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest metrics (GA + mandatory features)    Accuracy  Precision    Recall        F1   ROC-AUC   TN  FP  FN  TP  \\\n",
      "0   0.96789   0.925926  0.943396  0.934579  0.990623  161   4   3  50   \n",
      "\n",
      "   Total features  \n",
      "0              33  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix)\n",
    "\n",
    "# Load GA-selected feature matrix (created earlier)\n",
    "data = np.load('GA_selected_features.npz')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Random Forest model\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "y_proba = rf.predict_proba(X_test)[:, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "metrics = pd.DataFrame([{\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_pred),\n",
    "    'F1': f1_score(y_test, y_pred),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_proba),\n",
    "    'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp,\n",
    "    'Total features': X.shape[1]\n",
    "}])\n",
    "\n",
    "print(\"Random Forest metrics (GA + mandatory features)\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b3939c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.844  →  Precision 0.976  Recall 0.774\n",
      "FP: 1  ⇦ used to be 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "\n",
    "# rf  = fitted Random-Forest\n",
    "# X_te, y_te = hold-out test from earlier\n",
    "\n",
    "proba = rf.predict_proba(X_test)[:, 1]\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba)\n",
    "\n",
    "# choose the smallest threshold with Precision ≥ 0.96\n",
    "target = 0.96\n",
    "best_idx = np.argmax((prec >= target) * rec)        # maximise recall under constraint\n",
    "best_thr = thr[best_idx]\n",
    "print(f\"Threshold {best_thr:.3f}  →  Precision {prec[best_idx]:.3f}  Recall {rec[best_idx]:.3f}\")\n",
    "\n",
    "# evaluate\n",
    "y_hat = (proba >= best_thr).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_hat).ravel()\n",
    "print(\"FP:\", fp, \" ⇦ used to be 4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53902c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         thr  precision  recall      F0.5\n",
      "0   0.001667   0.243119     1.0  0.286486\n",
      "1   0.001942   0.244240     1.0  0.287731\n",
      "2   0.002878   0.245370     1.0  0.288986\n",
      "3   0.003546   0.246512     1.0  0.290252\n",
      "4   0.003859   0.247664     1.0  0.291529\n",
      "5   0.004267   0.248826     1.0  0.292818\n",
      "6   0.005856   0.250000     1.0  0.294118\n",
      "7   0.005939   0.251185     1.0  0.295429\n",
      "8   0.006405   0.252381     1.0  0.296753\n",
      "9   0.006436   0.253589     1.0  0.298088\n",
      "10  0.006640   0.254808     1.0  0.299435\n",
      "11  0.007596   0.256039     1.0  0.300795\n",
      "12  0.007775   0.257282     1.0  0.302166\n",
      "13  0.007835   0.258537     1.0  0.303551\n",
      "14  0.008125   0.259804     1.0  0.304948\n"
     ]
    }
   ],
   "source": [
    "proba = rf.predict_proba(X_test)[:, 1]\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba)\n",
    "\n",
    "# put everything in a tbale\n",
    "table = pd.DataFrame({\"thr\": thr, \"precision\": prec[:-1], \"recall\": rec[:-1]})\n",
    "table[\"F0.5\"] = 1.25 * prec[:-1] * rec[:-1] / (0.25 * prec[:-1] + rec[:-1])\n",
    "print(table.head(15))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb4e68f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          thr  precision    recall      F0.5\n",
      "166  0.713346   0.942308  0.924528  0.938697\n",
      "167  0.724686   0.941176  0.905660  0.933852\n",
      "168  0.767744   0.940000  0.886792  0.928854\n",
      "169  0.796650   0.938776  0.867925  0.923695\n",
      "170  0.797967   0.958333  0.867925  0.938776\n",
      "171  0.805829   0.957447  0.849057  0.933610\n",
      "172  0.810068   0.956522  0.830189  0.928270\n",
      "173  0.816920   0.955556  0.811321  0.922747\n",
      "174  0.833576   0.954545  0.792453  0.917031\n",
      "175  0.835978   0.953488  0.773585  0.911111\n"
     ]
    }
   ],
   "source": [
    "hi_prec  = table.query(\"precision >= 0.93\")\n",
    "print(hi_prec.head(10))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5762fb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF w/ threshold=0.713346\n",
      "Accuracy  : 0.9633\n",
      "Precision : 0.9412\n",
      "Recall    : 0.9057\n",
      "F1        : 0.9231\n",
      "ROC-AUC   : 0.9906\n",
      "TN        : 162\n",
      "FP        : 3\n",
      "FN        : 5\n",
      "TP        : 48\n",
      "\n",
      "Confusion matrix:\n",
      "          Pred 0  Pred 1\n",
      "Actual 0     162       3\n",
      "Actual 1       5      48\n"
     ]
    }
   ],
   "source": [
    "# RF on GA selected 33 features with a 0.71 threshold\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# Load the ready dataset and the 33 GA‐selected column names\n",
    "df      = pd.read_csv(\"domains_feature_ready.csv\")\n",
    "ga_cols = pd.read_csv(\"GA_selected_columns.txt\", header=None)[0].tolist()\n",
    "X       = df[ga_cols].values\n",
    "y       = df[\"label\"].values\n",
    "\n",
    "# Impute any missing → 0, then train/test split\n",
    "imp   = SimpleImputer(strategy=\"constant\", fill_value=0.0)\n",
    "X_imp = imp.fit_transform(X)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_imp, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the baseline RandomForest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=600,\n",
    "    class_weight=\"balanced\",\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ").fit(X_tr, y_tr)\n",
    "\n",
    "# Predict probabilities and apply threshold = 0.71\n",
    "y_proba     = rf.predict_proba(X_te)[:, 1]\n",
    "threshold   = 0.713346\n",
    "y_pred_thr  = (y_proba >= threshold).astype(int)\n",
    "\n",
    "# Compute metrics & confusion\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_pred_thr).ravel()\n",
    "metrics = {\n",
    "    \"Accuracy\":  accuracy_score(y_te, y_pred_thr),\n",
    "    \"Precision\": precision_score(y_te, y_pred_thr),\n",
    "    \"Recall\":    recall_score(y_te, y_pred_thr),\n",
    "    \"F1\":        f1_score(y_te, y_pred_thr),\n",
    "    \"ROC-AUC\":   roc_auc_score(y_te, y_proba),\n",
    "    \"TN\": tn, \"FP\": fp, \"FN\": fn, \"TP\": tp\n",
    "}\n",
    "\n",
    "print(f\"RF w/ threshold={threshold}\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k:<10}: {v:.4f}\" if isinstance(v, float) else f\"{k:<10}: {v}\")\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(pd.DataFrame(\n",
    "    [[tn, fp], [fn, tp]],\n",
    "    index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Pred 0\", \"Pred 1\"]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341f2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4a42a8b",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "946bb95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Features': 33, 'Accuracy': 0.944954128440367, 'Precision': 0.8360655737704918, 'Recall': 0.9622641509433962, 'F1': 0.8947368421052632, 'ROC-AUC': 0.9918810748999428, 'FP / TP': (10, 51), 'Best C': 1, 'Best γ': 0.01}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix)\n",
    "\n",
    "# ── 1. Load numeric feature matrix & GA column list ────────────────────\n",
    "df          = pd.read_csv(\"domains_feature_ready.csv\")\n",
    "ga_cols     = pd.read_csv(\"GA_selected_columns.txt\", header=None)[0].tolist()\n",
    "X_ga, y     = df[ga_cols], df[\"label\"]\n",
    "\n",
    "# ── 2. Preprocess & SVM classifier  ────────────────────────────────────\n",
    "svm_pipe = Pipeline([\n",
    "    (\"imp\",   SimpleImputer(strategy=\"constant\", fill_value=0.0)),\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"svm\",   SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"svm__C\":     [1, 5, 10],\n",
    "    \"svm__gamma\": [\"scale\", 0.1, 0.01]\n",
    "}\n",
    "\n",
    "svm = GridSearchCV(svm_pipe, param_grid, cv=3, scoring=\"roc_auc\", n_jobs=-1)\n",
    "\n",
    "# ── 3. Train/test split & fit ──────────────────────────────────────────\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_ga, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "svm.fit(X_tr, y_tr)\n",
    "\n",
    "# ── 4. Metrics ─────────────────────────────────────────────────────────\n",
    "y_pr  = svm.predict(X_te)\n",
    "y_pb  = svm.predict_proba(X_te)[:, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_pr).ravel()\n",
    "\n",
    "metrics = {\n",
    "    \"Features\":   len(ga_cols),\n",
    "    \"Accuracy\":   accuracy_score(y_te, y_pr),\n",
    "    \"Precision\":  precision_score(y_te, y_pr, zero_division=0),\n",
    "    \"Recall\":     recall_score(y_te, y_pr),\n",
    "    \"F1\":         f1_score(y_te, y_pr),\n",
    "    \"ROC-AUC\":    roc_auc_score(y_te, y_pb),\n",
    "    \"FP / TP\":    (fp, tp),\n",
    "    \"Best C\":     svm.best_params_[\"svm__C\"],\n",
    "    \"Best γ\":     svm.best_params_[\"svm__gamma\"],\n",
    "}\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad83f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b919f2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best params: {'svm__C': 1, 'svm__class_weight': {0: 1, 1: 1}, 'svm__gamma': 0.01}\n",
      "Chosen threshold = 0.548  (prec=0.907, rec=0.925)\n",
      "\n",
      "=== Final metrics ===\n",
      "Accuracy : 0.959\n",
      "Precision: 0.907\n",
      "Recall   : 0.925\n",
      "F1       : 0.916\n",
      "ROC-AUC  : 0.990\n",
      "TN / FP / FN / TP : 160 / 5 / 4 / 49\n",
      "\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred 0</th>\n",
       "      <th>Pred 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>160</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pred 0  Pred 1\n",
       "Actual 0     160       5\n",
       "Actual 1       4      49"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ── 1. Load data ──────────────────────────────────────────────────────────\n",
    "df      = pd.read_csv(\"domains_feature_ready.csv\")\n",
    "ga_cols = pd.read_csv(\"GA_selected_columns.txt\", header=None)[0].tolist()\n",
    "\n",
    "X = df[ga_cols]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# ── 2. Split ─────────────────────────────────────────────────────────────\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ── 3. Build pipeline ────────────────────────────────────────────────────\n",
    "pipe = Pipeline([\n",
    "    (\"imp\",   SimpleImputer(strategy=\"constant\", fill_value=0.0)),\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"svm\",   SVC(kernel=\"rbf\", probability=True))\n",
    "])\n",
    "\n",
    "# ── 4. Grid over C, γ **and** class-weight ───────────────────────────────\n",
    "param_grid = {\n",
    "    \"svm__C\":            [1, 5, 10],\n",
    "    \"svm__gamma\":        [\"scale\", 0.1, 0.01],\n",
    "    # penalize false positives by giving class 1 (positives) higher weight\n",
    "    \"svm__class_weight\": [\n",
    "        {0:1,   1:1},    # baseline\n",
    "        {0:1,   1:2},\n",
    "        {0:1,   1:5},\n",
    "        \"balanced\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe, param_grid, cv=3,\n",
    "    scoring=\"roc_auc\", n_jobs=-1, verbose=1\n",
    ")\n",
    "gs.fit(X_tr, y_tr)\n",
    "\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "\n",
    "# ── 5. Calibrate probabilities ───────────────────────────────────────────\n",
    "cal = CalibratedClassifierCV(\n",
    "    gs.best_estimator_, method=\"isotonic\", cv=3\n",
    ").fit(X_tr, y_tr)\n",
    "\n",
    "proba = cal.predict_proba(X_te)[:, 1]\n",
    "\n",
    "# ── 6. Pick threshold for prec ≥ 0.90 but max recall ────────────────────\n",
    "prec, rec, thr = precision_recall_curve(y_te, proba)\n",
    "# drop last point where thr is NAN\n",
    "prec, rec = prec[:-1], rec[:-1]\n",
    "thr        = thr\n",
    "\n",
    "mask = prec >= 0.90\n",
    "best_i = np.argmax(rec[mask])\n",
    "best_thr = thr[mask][best_i]\n",
    "print(f\"Chosen threshold = {best_thr:.3f}  \"\n",
    "      f\"(prec={prec[mask][best_i]:.3f}, rec={rec[mask][best_i]:.3f})\")\n",
    "\n",
    "# ── 7. Final prediction & metrics ───────────────────────────────────────\n",
    "y_pred = (proba >= best_thr).astype(int)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_pred).ravel()\n",
    "\n",
    "print(\"\\n=== Final metrics ===\")\n",
    "print(f\"Accuracy : {accuracy_score(y_te, y_pred):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_te, y_pred):.3f}\")\n",
    "print(f\"Recall   : {recall_score(y_te, y_pred):.3f}\")\n",
    "print(f\"F1       : {f1_score(y_te, y_pred):.3f}\")\n",
    "print(f\"ROC-AUC  : {roc_auc_score(y_te, proba):.3f}\")\n",
    "print(f\"TN / FP / FN / TP : {tn} / {fp} / {fn} / {tp}\")\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "display(pd.DataFrame(\n",
    "    [[tn, fp], [fn, tp]],\n",
    "    index=[\"Actual 0\",\"Actual 1\"],\n",
    "    columns=[\"Pred 0\",\"Pred 1\"]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3985ece",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "180251dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Features': 33, 'Accuracy': 0.963302752293578, 'Precision': 0.8813559322033898, 'Recall': 0.9811320754716981, 'F1': 0.9285714285714285, 'ROC-AUC': 0.9901658090337335, 'FP / TP': (7, 52), 'Best C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix)\n",
    "\n",
    "# ── 1. Load GA feature matrix ──────────────────────────────────────────\n",
    "df       = pd.read_csv(\"domains_feature_ready.csv\")\n",
    "ga_cols  = pd.read_csv(\"GA_selected_columns.txt\", header=None)[0].tolist()\n",
    "X_ga, y  = df[ga_cols], df[\"label\"]\n",
    "\n",
    "# ── 2. Build LR pipeline (impute → scale → LR) ─────────────────────────\n",
    "lr_pipe = Pipeline([\n",
    "    (\"imp\",   SimpleImputer(strategy=\"constant\", fill_value=0.0)),\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"lr\",    LogisticRegression(penalty=\"l2\", solver=\"saga\",\n",
    "                                 class_weight=\"balanced\",\n",
    "                                 max_iter=5000))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"lr__C\": [0.1, 0.3, 1, 3, 10]     # strength of regularisation\n",
    "}\n",
    "\n",
    "lr = GridSearchCV(lr_pipe, param_grid, cv=3,\n",
    "                  scoring=\"roc_auc\", n_jobs=-1, verbose=0)\n",
    "\n",
    "# ── 3. Train/test split & fit ──────────────────────────────────────────\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_ga, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "lr.fit(X_tr, y_tr)\n",
    "\n",
    "# ── 4. Metrics ─────────────────────────────────────────────────────────\n",
    "y_pr  = lr.predict(X_te)\n",
    "y_pb  = lr.predict_proba(X_te)[:, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_pr).ravel()\n",
    "\n",
    "metrics = {\n",
    "    \"Features\":  len(ga_cols),\n",
    "    \"Accuracy\":  accuracy_score(y_te, y_pr),\n",
    "    \"Precision\": precision_score(y_te, y_pr, zero_division=0),\n",
    "    \"Recall\":    recall_score(y_te, y_pr),\n",
    "    \"F1\":        f1_score(y_te, y_pr),\n",
    "    \"ROC-AUC\":   roc_auc_score(y_te, y_pb),\n",
    "    \"FP / TP\":   (fp, tp),\n",
    "    \"Best C\":    lr.best_params_[\"lr__C\"]\n",
    "}\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902b7c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ae802ab",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0e824a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Features': 33, 'Accuracy': 0.944954128440367, 'Precision': 0.9361702127659575, 'Recall': 0.8301886792452831, 'F1': 0.88, 'ROC-AUC': 0.9872498570611778, 'FP / TP': (3, 44), 'Best k': 25, 'Metric': 'manhattan'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix)\n",
    "\n",
    "# ── 1.  Load GA feature matrix ─────────────────────────────────────────\n",
    "df       = pd.read_csv(\"domains_feature_ready.csv\")\n",
    "ga_cols  = pd.read_csv(\"GA_selected_columns.txt\", header=None)[0].tolist()\n",
    "X_ga, y  = df[ga_cols], df[\"label\"]\n",
    "\n",
    "# ── 2.  KNN pipeline: impute → scale → KNN ─────────────────────────────\n",
    "knn_pipe = Pipeline([\n",
    "    (\"imp\",   SimpleImputer(strategy=\"constant\", fill_value=0.0)),\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"knn\",   KNeighborsClassifier(weights=\"uniform\"))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"knn__n_neighbors\": [3, 5, 10, 15, 20, 25, 30],\n",
    "    \"knn__metric\":      [\"minkowski\", \"euclidean\", \"manhattan\"]\n",
    "}\n",
    "\n",
    "knn = GridSearchCV(knn_pipe, param_grid,\n",
    "                   cv=3, scoring=\"roc_auc\", n_jobs=-1, verbose=0)\n",
    "\n",
    "# ── 3.  Train / test split & fit ───────────────────────────────────────\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_ga, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "knn.fit(X_tr, y_tr)\n",
    "\n",
    "# ── 4.  Metrics ────────────────────────────────────────────────────────\n",
    "y_pr = knn.predict(X_te)\n",
    "y_pb = knn.predict_proba(X_te)[:, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_pr).ravel()\n",
    "\n",
    "metrics = {\n",
    "    \"Features\":   len(ga_cols),\n",
    "    \"Accuracy\":   accuracy_score(y_te, y_pr),\n",
    "    \"Precision\":  precision_score(y_te, y_pr, zero_division=0),\n",
    "    \"Recall\":     recall_score(y_te, y_pr),\n",
    "    \"F1\":         f1_score(y_te, y_pr),\n",
    "    \"ROC-AUC\":    roc_auc_score(y_te, y_pb),\n",
    "    \"FP / TP\":    (fp, tp),\n",
    "    \"Best k\":     knn.best_params_[\"knn__n_neighbors\"],\n",
    "    \"Metric\":     knn.best_params_[\"knn__metric\"]\n",
    "}\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813a6e79",
   "metadata": {},
   "source": [
    "# NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5914d06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Features': 33, 'Accuracy': 0.9311926605504587, 'Precision': 0.796875, 'Recall': 0.9622641509433962, 'F1': 0.8717948717948717, 'ROC-AUC': 0.9618067467124072, 'FP / TP': (13, 51)}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix)\n",
    "\n",
    "# ── 1. Load GA feature matrix ──────────────────────────────────────────\n",
    "df       = pd.read_csv(\"domains_feature_ready.csv\")\n",
    "ga_cols  = pd.read_csv(\"GA_selected_columns.txt\", header=None)[0].tolist()\n",
    "X_ga, y  = df[ga_cols], df[\"label\"]\n",
    "\n",
    "# ── 2. Build pipeline: impute → GaussianNB  ────────────────────────────\n",
    "nb_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=0.0)),\n",
    "    (\"gnb\", GaussianNB())\n",
    "])\n",
    "\n",
    "# ── 3. Train/test split & fit ──────────────────────────────────────────\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_ga, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "nb_pipe.fit(X_tr, y_tr)\n",
    "\n",
    "# ── 4. Metrics ─────────────────────────────────────────────────────────\n",
    "y_pr = nb_pipe.predict(X_te)\n",
    "y_pb = nb_pipe.predict_proba(X_te)[:, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_pr).ravel()\n",
    "\n",
    "metrics = {\n",
    "    \"Features\":  len(ga_cols),\n",
    "    \"Accuracy\":  accuracy_score(y_te, y_pr),\n",
    "    \"Precision\": precision_score(y_te, y_pr, zero_division=0),\n",
    "    \"Recall\":    recall_score(y_te, y_pr),\n",
    "    \"F1\":        f1_score(y_te, y_pr),\n",
    "    \"ROC-AUC\":   roc_auc_score(y_te, y_pb),\n",
    "    \"FP / TP\":   (fp, tp)\n",
    "}\n",
    "print(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
